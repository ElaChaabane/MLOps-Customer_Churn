pipeline {
    agent any  // Runs the pipeline on any available Jenkins agent

    // Define environment variables used throughout the pipeline
    environment {
        VENV_DIR = 'venv'  // Name of the virtual environment directory
        TRAINING_SCRIPT = 'main.py'  // Main script for data processing and model training
        MODEL_FILE = 'churn_model.joblib'  // Model output file
        IMAGE_NAME = 'asinet/mlflow-server'  // Docker image name for MLflow
        TAG = 'latest'  // Docker image tag
        MLFLOW_PORT = '5000'  // Port to run MLflow
        MLFLOW_BACKEND_STORE = 'sqlite:///mlflow.db'  // SQLite database for MLflow tracking
        MLFLOW_ARTIFACT_ROOT = './mlruns'  // Directory for MLflow artifacts
        // Define the correct data path relative to workspace
        DATA_PATH = './churn-bigml-80.csv'  // Add this line to specify where data should be located
    }

    stages {  // Pipeline execution stages

        stage('Checkout') {  
            steps {
                // Simple checkout for public repository
                checkout scm
            }
        }

        stage('Install Dependencies') {
            steps {
                script {
                    sh '''#!/bin/bash
                        echo "üîÑ Installing dependencies..."
                        python3 -m venv ${VENV_DIR}  # Create a virtual environment
                        . ${VENV_DIR}/bin/activate  # Activate the virtual environment using dot instead of source
                        pip install --upgrade pip setuptools wheel  # Upgrade essential Python tools
                        pip install -r requirements.txt  # Install dependencies
                    '''
                }
            }
        }

      
        stage('Prepare Data') {
            steps {
                script {
                    // Create data directory if it doesn't exist and ensure file is available
                    sh '''#!/bin/bash
                        mkdir -p ${DATA_PATH}
                        
                        # Check if the data file exists, if not, download or copy it from a reliable source
                        if [ ! -f "${DATA_PATH}/churn-bigml-80.csv" ]; then
                            echo "‚ö†Ô∏è Data file not found. Attempting to fetch it..."
                            # Example: Copy from a known location or download it
                             cp  ./churn-bigml-80.csv ${DATA_PATH}/
                            # OR download using curl/wget:
                            # curl -o ${DATA_PATH}/churn-bigml-80.csv https://example.com/data/churn-bigml-80.csv
                        fi
                        
                        # Run the preparation script with the correct path
                        . ${VENV_DIR}/bin/activate && python ${TRAINING_SCRIPT} --prepare --data-dir=${DATA_PATH}
                    '''
                }
            }
        }

        stage('Train Model') {
            steps {
                script {
                    // Runs model training with the correct data path
                    sh '''#!/bin/bash
                        . ${VENV_DIR}/bin/activate && python ${TRAINING_SCRIPT} --train --data-dir=${DATA_PATH}
                    '''
                }
            }
        }

        stage('Run Tests') {
            steps {
                script {
                    // Runs unit tests on the model pipeline
                    sh '''#!/bin/bash
                        . ${VENV_DIR}/bin/activate && pytest tests/test_model_pipeline.py -v
                    '''
                }
            }
        }

        stage('Evaluate Model') {
            steps {
                script {
                    // Evaluates model performance with the correct data path
                    sh '''#!/bin/bash
                        . ${VENV_DIR}/bin/activate && python ${TRAINING_SCRIPT} --evaluate --data-dir=${DATA_PATH}
                    '''
                }
            }
        }

        stage('Save Model') {
            steps {
                script {
                    // Saves the trained model to a file
                    sh '''#!/bin/bash
                        . ${VENV_DIR}/bin/activate && python ${TRAINING_SCRIPT} --save ${MODEL_FILE}
                    '''
                }
            }
        }

        stage('Build Docker Image') {
            steps {
                script {
                    // Builds a Docker image containing the ML model
                    sh "docker build -t ${IMAGE_NAME}:${TAG} ."
                }
            }
        }

        stage('Push Docker Image') {
            steps {
                script {
                    // Pushes the Docker image to a container registry
                    sh "docker push ${IMAGE_NAME}:${TAG}"
                }
            }
        }

        stage('Deploy MLflow Server') {
            steps {
                script {
                    sh '''#!/bin/bash
                        echo "üîÑ Starting MLflow Server..."
                        . ${VENV_DIR}/bin/activate && mlflow server \
                            --backend-store-uri ${MLFLOW_BACKEND_STORE} \
                            --default-artifact-root ${MLFLOW_ARTIFACT_ROOT} \
                            --host 0.0.0.0 \
                            --port ${MLFLOW_PORT} &
                    '''
                }
            }
        }
    }
}